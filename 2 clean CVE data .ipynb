{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "986812ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7656334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nltk.__version__) #trying to remember if i installed nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b39efc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/angelesmarin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "321a6c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/angelesmarin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a24be9c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/angelesmarin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') #req for lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4c88226",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ('/Users/angelesmarin/Desktop/CVE.csv')\n",
    "df = pd.read_csv(path, encoding='ISO-8859-1', low_memory=False)\n",
    "#was getting an error trying to decode using utf-8; got warrning to set low_memory=False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b0d70",
   "metadata": {},
   "source": [
    "### dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dac2f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Phase', 'Votes', 'Comments'], axis=1)\n",
    "#dropped outdated/ legacy fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9b929",
   "metadata": {},
   "source": [
    "#### References has numbers, so just making the whole thing a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32d8f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['References'] = df['References'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c0c3342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the dataframe: 4\n"
     ]
    }
   ],
   "source": [
    "#df_columns = df.shape[1]  #columns\n",
    "#print(f\"num of columns: {df_columns}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b68e35",
   "metadata": {},
   "source": [
    "### check for null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c7b08ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                        0\n",
      "Status                      0\n",
      "Description                 0\n",
      "References                  0\n",
      "Tokenized_Description       0\n",
      "Tokenized_Reference         0\n",
      "Description_No_Stopwords    0\n",
      "Lemmatize_Description       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c684b",
   "metadata": {},
   "source": [
    "### tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3eb435e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_description(desc):\n",
    "    return word_tokenize(desc)\n",
    "df['Tokenized_Description'] = df['Description'].apply(tokenize_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33687f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Description  \\\n",
      "0  ip_input.c in BSD-derived TCP/IP implementatio...   \n",
      "1  Buffer overflow in NFS mountd gives root acces...   \n",
      "2  Execute commands as root via buffer overflow i...   \n",
      "\n",
      "                               Tokenized_Description  \n",
      "0  [ip_input.c, in, BSD-derived, TCP/IP, implemen...  \n",
      "1  [Buffer, overflow, in, NFS, mountd, gives, roo...  \n",
      "2  [Execute, commands, as, root, via, buffer, ove...  \n"
     ]
    }
   ],
   "source": [
    "print(df[['Description', 'Tokenized_Description']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# references "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73ab8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reference(ref):\n",
    "    references = ref.split('|') #delimeter '|'\n",
    "    tokenized_references = [r.split('::') for r in references] #delimeter ':'\n",
    "    return tokenized_references\n",
    "df['Tokenized_Reference'] = df['References'].apply(tokenize_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de242905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          References  \\\n",
      "0  BUGTRAQ:19981223 Re: CERT Advisory CA-98.13 - ...   \n",
      "1  BID:121   |   URL:http://www.securityfocus.com...   \n",
      "2  BID:122   |   URL:http://www.securityfocus.com...   \n",
      "\n",
      "                                 Tokenized_Reference  \n",
      "0  [[BUGTRAQ:19981223 Re: CERT Advisory CA-98.13 ...  \n",
      "1  [[BID:121   ], [   URL:http://www.securityfocu...  \n",
      "2  [[BID:122   ], [   URL:http://www.securityfocu...  \n"
     ]
    }
   ],
   "source": [
    "print(df[['References', 'Tokenized_Reference']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998c103",
   "metadata": {},
   "source": [
    "### remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09821714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "322ff64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "669fd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_desc):\n",
    "    return [word for word in tokenized_desc if word.lower() not in stop_words]\n",
    "df['Description_No_Stopwords'] = df['Tokenized_Description'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8e24523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Description_No_Stopwords\n",
      "0  [ip_input.c, BSD-derived, TCP/IP, implementati...\n",
      "1  [Buffer, overflow, NFS, mountd, gives, root, a...\n",
      "2  [Execute, commands, root, via, buffer, overflo...\n"
     ]
    }
   ],
   "source": [
    "print(df[['Description_No_Stopwords']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150a30f",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1661ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f85f7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization = WordNetLemmatizer()\n",
    "def lemmatize_words(tokenized_desc):\n",
    "    return [lemmatization.lemmatize(word) for word in tokenized_desc]\n",
    "df['Lemmatize_Description'] = df['Description_No_Stopwords'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbf37ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Lemmatize_Description\n",
      "0  [ip_input.c, BSD-derived, TCP/IP, implementati...\n",
      "1  [Buffer, overflow, NFS, mountd, give, root, ac...\n",
      "2  [Execute, command, root, via, buffer, overflow...\n"
     ]
    }
   ],
   "source": [
    "print(df[['Lemmatize_Description']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96bb2ee",
   "metadata": {},
   "source": [
    "### output dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80fd64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = df[['Name', 'Status', 'Lemmatize_Description', 'Tokenized_Reference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "711ea7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Name     Status  \\\n",
      "324604  CVE-2024-6320  Candidate   \n",
      "324605  CVE-2024-6321  Candidate   \n",
      "324606  CVE-2024-6322  Candidate   \n",
      "324607  CVE-2024-6323  Candidate   \n",
      "324608  CVE-2024-6324  Candidate   \n",
      "\n",
      "                                              Description References  \\\n",
      "324604  ** RESERVED ** This candidate has been reserve...        nan   \n",
      "324605  ** RESERVED ** This candidate has been reserve...        nan   \n",
      "324606  ** RESERVED ** This candidate has been reserve...        nan   \n",
      "324607  ** RESERVED ** This candidate has been reserve...        nan   \n",
      "324608  ** RESERVED ** This candidate has been reserve...        nan   \n",
      "\n",
      "                                    Tokenized_Description Tokenized_Reference  \\\n",
      "324604  [*, *, RESERVED, *, *, This, candidate, has, b...             [[nan]]   \n",
      "324605  [*, *, RESERVED, *, *, This, candidate, has, b...             [[nan]]   \n",
      "324606  [*, *, RESERVED, *, *, This, candidate, has, b...             [[nan]]   \n",
      "324607  [*, *, RESERVED, *, *, This, candidate, has, b...             [[nan]]   \n",
      "324608  [*, *, RESERVED, *, *, This, candidate, has, b...             [[nan]]   \n",
      "\n",
      "                                 Description_No_Stopwords  \\\n",
      "324604  [*, *, RESERVED, *, *, candidate, reserved, or...   \n",
      "324605  [*, *, RESERVED, *, *, candidate, reserved, or...   \n",
      "324606  [*, *, RESERVED, *, *, candidate, reserved, or...   \n",
      "324607  [*, *, RESERVED, *, *, candidate, reserved, or...   \n",
      "324608  [*, *, RESERVED, *, *, candidate, reserved, or...   \n",
      "\n",
      "                                    Lemmatize_Description  \n",
      "324604  [*, *, RESERVED, *, *, candidate, reserved, or...  \n",
      "324605  [*, *, RESERVED, *, *, candidate, reserved, or...  \n",
      "324606  [*, *, RESERVED, *, *, candidate, reserved, or...  \n",
      "324607  [*, *, RESERVED, *, *, candidate, reserved, or...  \n",
      "324608  [*, *, RESERVED, *, *, candidate, reserved, or...  \n"
     ]
    }
   ],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "035cc983",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe.to_csv('/Users/angelesmarin/Desktop/output_dataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c6eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
